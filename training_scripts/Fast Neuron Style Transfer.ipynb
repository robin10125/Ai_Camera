{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c459ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.applications import vgg19\n",
    "from tensorflow.keras.models import load_model,Model\n",
    "from PIL import Image\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "matplotlib.rcParams['figure.figsize'] = (12,12)\n",
    "matplotlib.rcParams['axes.grid'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407f215b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path, dim=None, resize=False):\n",
    "    img= Image.open(image_path)\n",
    "    if dim:\n",
    "        if resize:\n",
    "            img=img.resize(dim)\n",
    "        else:\n",
    "            img.thumbnail(dim)\n",
    "    img= img.convert(\"RGB\")\n",
    "    return np.array(img)\n",
    "\n",
    "def array_to_img(array):\n",
    "    array=np.array(array,dtype=np.uint8)\n",
    "    if np.ndim(array)>3:\n",
    "        assert array.shape[0]==1\n",
    "        array=array[0]\n",
    "    return Image.fromarray(array)\n",
    "\n",
    "def show_image(image,title=None):\n",
    "    if len(image.shape)>3:\n",
    "        image=tf.squeeze(image,axis=0)\n",
    "    plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598bd3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg=vgg19.VGG19(weights='imagenet',include_top=False)\n",
    "vgg.summary()\n",
    "\n",
    "content_layers=['block4_conv2']\n",
    "\n",
    "style_layers=['block1_conv1',\n",
    "            'block2_conv1',\n",
    "            'block3_conv1',\n",
    "            'block4_conv1',\n",
    "            'block5_conv1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a213f454",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossModel:\n",
    "    def __init__(self,pretrained_model,content_layers,style_layers):\n",
    "        self.model=pretrained_model\n",
    "        self.content_layers=content_layers\n",
    "        self.style_layers=style_layers\n",
    "        self.loss_model=self.get_model()\n",
    "\n",
    "    def get_model(self):\n",
    "        self.model.trainable=False\n",
    "        layer_names=self.style_layers + self.content_layers\n",
    "        outputs=[self.model.get_layer(name).output for name in layer_names]\n",
    "        new_model=Model(inputs=self.model.input,outputs=outputs)\n",
    "        return new_model\n",
    "    \n",
    "    def get_activations(self,inputs):\n",
    "        inputs=inputs*255.0\n",
    "        style_length=len(self.style_layers)\n",
    "        outputs=self.loss_model(vgg19.preprocess_input(inputs))\n",
    "        style_output,content_output=outputs[:style_length],outputs[style_length:]\n",
    "        content_dict={name:value for name,value in zip(self.content_layers,content_output)}\n",
    "        style_dict={name:value for name,value in zip(self.style_layers,style_output)}\n",
    "        return {'content':content_dict,'style':style_dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6af1b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_model = LossModel(vgg, content_layers, style_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b34621f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def content_loss(placeholder,content,weight):\n",
    "    assert placeholder.shape == content.shape\n",
    "    return weight*tf.reduce_mean(tf.square(placeholder-content))\n",
    "\n",
    "def gram_matrix(x):\n",
    "    gram=tf.linalg.einsum('bijc,bijd->bcd', x, x)\n",
    "    return gram/tf.cast(x.shape[1]*x.shape[2]*x.shape[3],tf.float32)\n",
    "\n",
    "def style_loss(placeholder,style, weight):\n",
    "    assert placeholder.shape == style.shape\n",
    "    s=gram_matrix(style)\n",
    "    p=gram_matrix(placeholder)\n",
    "    return weight*tf.reduce_mean(tf.square(s-p))\n",
    "\n",
    "def perceptual_loss(predicted_activations,content_activations,\n",
    "                    style_activations,content_weight,style_weight,\n",
    "                    content_layers_weights,style_layer_weights):\n",
    "    pred_content = predicted_activations[\"content\"]\n",
    "    pred_style = predicted_activations[\"style\"]\n",
    "    c_loss = tf.add_n([content_loss(pred_content[name],content_activations[name],\n",
    "                                  content_layers_weights[i]) for i,name in enumerate(pred_content.keys())])\n",
    "    c_loss = c_loss*content_weight\n",
    "    s_loss = tf.add_n([style_loss(pred_style[name],style_activations[name],\n",
    "                                style_layer_weights[i]) for i,name in enumerate(pred_style.keys())])\n",
    "    s_loss = s_loss*style_weight\n",
    "    return c_loss+s_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c803ca7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet_block(num_filters, input_layer):\n",
    "    init = tf.random_normal_initializer(0., 0.02)\n",
    "    # first convolutional layer\n",
    "    g = tf.keras.layers.Conv2D(num_filters, (3,3), padding='same', kernel_initializer=init)(input_layer)\n",
    "    g = tfa.layers.InstanceNormalization(axis=-1)(g)\n",
    "    g = tf.keras.layers.Activation('relu')(g)\n",
    "    # second convolutional layer\n",
    "    g = tf.keras.layers.Conv2D(num_filters, (3,3), padding='same', kernel_initializer=init)(g)\n",
    "    g = tfa.layers.InstanceNormalization(axis=-1)(g)\n",
    "    # concatenate with input layer\n",
    "    g = tf.keras.layers.Concatenate()([g, input_layer])\n",
    "    return g\n",
    "def generator():\n",
    " \n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "    inputs = tf.keras.layers.Input(shape=[256, 256, 3])\n",
    "    \n",
    "    #first encoding layer\n",
    "    x = tf.keras.layers.Conv2D(32, (9, 9), padding=\"same\", kernel_initializer = initializer)(inputs)\n",
    "    x= tfa.layers.InstanceNormalization(axis = -1)(x)\n",
    "    x = tf.keras.layers.Activation('relu')(x)\n",
    "    #second encoding layer\n",
    "    x = tf.keras.layers.Conv2D(64, (3, 3), strides = 2, padding=\"same\", kernel_initializer = initializer)(x)\n",
    "    x= tfa.layers.InstanceNormalization(axis = -1)(x)\n",
    "    x = tf.keras.layers.Activation('relu')(x)\n",
    "    #third encoding layer\n",
    "    x = tf.keras.layers.Conv2D(128, (3, 3), strides = 2, padding=\"same\", kernel_initializer = initializer)(x)\n",
    "    x= tfa.layers.InstanceNormalization(axis = -1)(x)\n",
    "    x = tf.keras.layers.Activation('relu')(x)\n",
    "    #resnet blocks\n",
    "    for _ in range(5):\n",
    "        x = resnet_block(128, x)\n",
    "    #first decoding layer\n",
    "    x = tf.keras.layers.Conv2DTranspose(64, (3, 3), strides = 2, name = \"feature_map\", padding=\"same\", kernel_initializer = initializer)(x)\n",
    "    x= tfa.layers.InstanceNormalization(axis = -1)(x)\n",
    "    x = tf.keras.layers.Activation('relu')(x)\n",
    "    #second decoding layer\n",
    "    x = tf.keras.layers.Conv2DTranspose(32, (3, 3), strides = 2, padding=\"same\", kernel_initializer = initializer)(x)\n",
    "    x= tfa.layers.InstanceNormalization(axis = -1)(x)\n",
    "    x = tf.keras.layers.Activation('relu')(x)\n",
    "    #third decoding layewr\n",
    "    x = tf.keras.layers.Conv2D(3, (9, 9), padding=\"same\", kernel_initializer = initializer)(x)\n",
    "    x= tfa.layers.InstanceNormalization(axis = -1)(x)\n",
    "    output_image = (tf.keras.layers.Activation('tanh')(x) + 1) * (255.0 / 2)\n",
    "    \n",
    "    model = tf.keras.Model(inputs, output_image)\n",
    "    return model\n",
    "test_model = generator()\n",
    "test_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c1fd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape=(256,256,3)\n",
    "batch_size=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2f7f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948f2af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(dataset,style_activations,steps_per_epoch,style_model,loss_model,optimizer,\n",
    "               checkpoint_path=\"./\",content_weight=1e4, style_weight=1e-2,\n",
    "               total_variation_weight=0.004):\n",
    "    \n",
    "    content_layers_weights=[1]\n",
    "    style_layers_weights=[1,1,1,1,1]\n",
    "    batch_losses=[]\n",
    "    steps=1\n",
    "    save_path=os.path.join(checkpoint_path, \"model_checkpoint.ckpt\")\n",
    "    print(\"Model Checkpoint Path: \",save_path)\n",
    "    for input_image_batch in dataset:\n",
    "        if steps-1 >= steps_per_epoch:\n",
    "            break\n",
    "        with tf.GradientTape() as tape:\n",
    "            outputs=style_model(input_image_batch)\n",
    "            outputs=tf.clip_by_value(outputs, 0, 255)\n",
    "            pred_activations=loss_model.get_activations(outputs/255.0)\n",
    "            content_activations=loss_model.get_activations(input_image_batch)[\"content\"] \n",
    "            curr_loss=perceptual_loss(pred_activations,content_activations,style_activations,content_weight,\n",
    "                                      style_weight,content_layers_weights,style_layers_weights)\n",
    "            curr_loss += total_variation_weight*tf.image.total_variation(outputs)\n",
    "        batch_losses.append(curr_loss)\n",
    "        grad = tape.gradient(curr_loss,style_model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grad,style_model.trainable_variables))\n",
    "        if steps % 1000==0:\n",
    "            print(\"checkpoint saved \",end=\" \")\n",
    "            test_model.save_weights(save_path)\n",
    "            print(f\"Loss: {tf.reduce_mean(batch_losses).numpy()}\")\n",
    "        steps+=1\n",
    "    return tf.reduce_mean(batch_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ee9972",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorflowDatasetLoader:\n",
    "    def __init__(self,dataset_path,batch_size=4, image_size=(256, 256),num_images=None):\n",
    "        images_paths = [str(path) for path in Path(dataset_path).glob(\"*.jpg\")]\n",
    "        self.length=len(images_paths)\n",
    "        if num_images is not None:\n",
    "            images_paths = images_paths[0:num_images]\n",
    "        dataset = tf.data.Dataset.from_tensor_slices(images_paths).map(\n",
    "            lambda path: self.load_tf_image(path, dim=image_size),\n",
    "            num_parallel_calls=tf.data.experimental.AUTOTUNE,\n",
    "        )\n",
    "        dataset = dataset.batch(batch_size,drop_remainder=True)\n",
    "        dataset = dataset.repeat()\n",
    "        dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "        self.dataset=dataset\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    def load_tf_image(self,image_path,dim):\n",
    "        image = tf.io.read_file(image_path)\n",
    "        image = tf.image.decode_jpeg(image, channels=3)\n",
    "        image= tf.image.resize(image,dim)\n",
    "        image= image/255.0\n",
    "        image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706696c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader=TensorflowDatasetLoader(\"C:/Users/robin/Downloads/train2014/train2014\",batch_size=4)\n",
    "loader.dataset.element_spec\n",
    "plot_images_grid(next(iter(loader.dataset.take(1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e014a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "url='path/to/style/image'\n",
    "#url=\"https://www.edvardmunch.org/images/paintings/the-scream.jpg\"\n",
    "style_image=load_image(url,dim=(input_shape[0],input_shape[1]),resize=True)\n",
    "style_image=style_image/255.0\n",
    "show_image(style_image)\n",
    "\n",
    "style_image=style_image.astype(np.float32)\n",
    "style_image_batch=np.repeat([style_image],batch_size,axis=0)\n",
    "style_activations=loss_model.get_activations(style_image_batch)[\"style\"]\n",
    "\n",
    "content_weight=1e1\n",
    "style_weight=1e2\n",
    "total_variation_weight=0.004\n",
    "epochs=2\n",
    "num_images=len(loader)\n",
    "steps_per_epochs=num_images//batch_size\n",
    "print(steps_per_epochs)\n",
    "save_path = \"./impressionism_painter3\"\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "if os.path.isfile(os.path.join(save_path,\"model_checkpoint.ckpt.index\")):\n",
    "    test_model.load_weights(os.path.join(save_path,\"model_checkpoint.ckpt\"))\n",
    "    print(\"resuming training ...\")\n",
    "else:\n",
    "    print(\"training scratch ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90502337",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_losses=[]\n",
    "for epoch in range(1,epochs+1):\n",
    "    print(f\"epoch: {epoch}\")\n",
    "    batch_loss=train_step(loader.dataset,style_activations,steps_per_epochs,test_model,loss_model,optimizer,\n",
    "                          save_path,\n",
    "                          content_weight,style_weight,total_variation_weight)\n",
    "    test_model.save_weights(os.path.join(save_path,\"model_checkpoint.ckpt\"))\n",
    "    print(\"Model Checkpointed at: \",os.path.join(save_path,\"model_checkpoint.ckpt\"))\n",
    "    print(f\"loss: {batch_loss.numpy()}\")\n",
    "    epoch_losses.append(batch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0d1446",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model.save('./impressionism_style_transfer')\n",
    "\n",
    "model = keras.models.load_model('impressionism_style_transfer',  compile=False)\n",
    "\n",
    "val_extractor = keras.Model(model.inputs,\n",
    "                        outputs=model.get_layer('activation_10').output)\n",
    "val_extractor.save('./impressionism_style_transfer01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5938e37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
